{"Smerity": ["As another Australian, our housing prices is broken for multiple reasons but foreign investment is a _major_ contributor.<p>Credit Suisse research indicates that &quot;foreigners are acquiring 25 per cent of newly completed supply in NSW and 16 per cent in Melbourne, or 21 per cent if we combine the two states&quot;. The total value of new houses in both states was $39 billion over the relevant 12 months.<p>After the Chinese government cracked down on monetary restrictions (i.e. Citizens of China can normally only convert US$50,000 a year in foreign currency and have long been barred from buying property overseas), Lend Lease reported 30 to 40% of foreign purchases now being cash settled.<p>Transparency International consider Australia the worst money laundering property market in the world.<p>Foreign investment, especially for countries which avoided the 2007 housing bubble such as Australia, is a major issue.<p>A friend will be posting an article on the multiple bubbles he sees in the Australian economy, and property is potentially the most concerning of all given how exposed our banks are to housing loans.\nWe may find Australia finally seeing their house bubble pop like the US in 2007.<p><a href=\"http:&#x2F;&#x2F;www.smh.com.au&#x2F;business&#x2F;property&#x2F;australian-property-cheap-for-chinese-buyers-credit-suisse-20170323-gv5ets.html\" rel=\"nofollow\">http:&#x2F;&#x2F;www.smh.com.au&#x2F;business&#x2F;property&#x2F;australian-property-...</a><p><a href=\"https:&#x2F;&#x2F;www.transparency.org&#x2F;whatwedo&#x2F;publication&#x2F;doors_wide_open_corruption_and_real_estate_in_four_key_markets\" rel=\"nofollow\">https:&#x2F;&#x2F;www.transparency.org&#x2F;whatwedo&#x2F;publication&#x2F;doors_wide...</a><p>Updated to include direct link to Credit Suisse: <a href=\"https:&#x2F;&#x2F;research-doc.credit-suisse.com&#x2F;docView?language=ENG&amp;format=PDF&amp;sourceid=csplusresearchcp&amp;document_id=1072773781&amp;serialid=vGaaRmOdA9BYZyKVi%2BOQvCfX%2BJZ8vpuaqcWf8o4NoF8%3D\" rel=\"nofollow\">https:&#x2F;&#x2F;research-doc.credit-suisse.com&#x2F;docView?language=ENG&amp;...</a>", "Amazon offer an official AMI which comes preloaded with various deep learning frameworks: MXNet, TensorFlow, CNTK, Caffe&#x2F;2, Theano, Torch and Keras.<p>For the P3 (Volta V100) instances you&#x27;ll want to ensure you use an AMI preloaded with CUDA 9, though not all DL frameworks are happy with that yet.<p><a href=\"https:&#x2F;&#x2F;aws.amazon.com&#x2F;amazon-ai&#x2F;amis&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;aws.amazon.com&#x2F;amazon-ai&#x2F;amis&#x2F;</a>", "Honestly, I didn&#x27;t spend enough time delving in to the Python overhead, especially in terms of the framework. Most of it would be an issue of my own causing however rather than the framework&#x27;s. The original code I wrote was never written with data loading &#x2F; saving in mind as the source for speed issues so I avoided what would have been premature optimization at the time.<p>Some of the slowdowns now just seem silly and aren&#x27;t even listed in the per epoch timings: PyTorch doesn&#x27;t have an asynchronous torch.save(). This means that if you save your model after each epoch, and the model save takes a few seconds, you&#x27;re increasing your per epoch timings 5-10% just by saving the damn thing!<p>Regarding FP16, PyTorch supports, and there&#x27;s even a pull request that updates the examples repo with FP16 support for language modeling and ImageNet. It&#x27;s not likely to be merged as it greatly complicates a codebase that&#x27;s meant primarily for teaching purposes but it&#x27;s lovely to look at. I also think many of the FP16 issues will get a general wrapper and they&#x27;ll become far more agnostic to the end user. For the most part they&#x27;re all outlined in NVIDIA &#x2F; Baidu&#x27;s &quot;Mixed Precision Training&quot; paper. Might be useful for DeepLearning4j to go through the most common heavy throughput use cases and get them running (just as an example of how to work around issues really) if customers were using P100s&#x2F;V100s?<p>I&#x27;m really interested in exploring the FP16 aspect as the QRNN, especially for single GPU, is sitting at basically 100% utilization, with almost all the time spent on matrix multiplications. FP16 is about the only way to speed it up at that stage. This gets a tad more complicated regardless as the CUDA kernel is not written in FP16 (and is not easy to do so) but even converting FP16-&gt;FP32-&gt;(QRNN element-wise CUDA kernel)-&gt;FP16 (&quot;pseudo&quot; FP16) should still be a crazy speedup. I tested that on the P100 and it took per epoch AWD-QRNN from ~28 seconds to ~18.<p>- PyTorch async save issue: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;pytorch&#x2F;issues&#x2F;1567\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;pytorch&#x2F;issues&#x2F;1567</a><p>- PyTorch FP16 examples pull request: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;examples&#x2F;pull&#x2F;203\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;examples&#x2F;pull&#x2F;203</a><p>- &quot;Mixed Precision Training&quot;: <a href=\"https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1710.03740\" rel=\"nofollow\">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1710.03740</a>", "The P3 instances are the first widely and easily accessible machines that use the NVIDIA Tesla V100 GPUs. These GPUs are straight up scary in terms of firepower.\nTo give an understanding of the speed-up compared to the P2 instances for a research project of mine:<p>+ P2 (K80) with single GPU: ~95 seconds per epoch<p>+ P3 (V100) with single GPU: ~20 seconds per epoch<p>Admittedly this isn&#x27;t exactly fair for either GPU - the K80 cards are straight up ancient now and the Volta isn&#x27;t sitting at 100% GPU utilization as it burns through the data too quickly ([CUDA kernel, Python] overhead suddenly become major bottlenecks).\nThis gives you an indication of what a leap this is if you&#x27;re using GPUs on AWS however.\nOh, and the V100 comes with 16GB of (faster) RAM compared to the K80&#x27;s 12GB of RAM, so you win there too.<p>For anyone using the standard set of frameworks (Tensorflow, Keras, PyTorch, Chainer, MXNet, DyNet, DeepLearning4j, ...) this type of speed-up will likely require you to do nothing - except throw more money at the P3 instance :)<p>If you really want to get into the black magic of speed-ups, these cards also feature full FP16 support, which means you can double your TFLOPS by dropping to FP16 from FP32. You&#x27;ll run into a million problems during training due to the lower precision but these aren&#x27;t insurmountable and may well be worth the pain for the additional speed-up &#x2F; better RAM usage.<p>- Good overview of Volta&#x27;s advantages compared to event the recent P100: <a href=\"https:&#x2F;&#x2F;devblogs.nvidia.com&#x2F;parallelforall&#x2F;inside-volta&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;devblogs.nvidia.com&#x2F;parallelforall&#x2F;inside-volta&#x2F;</a><p>- Simple table comparing V100 &#x2F; P100 &#x2F; K40 &#x2F; M40: <a href=\"https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;11367&#x2F;nvidia-volta-unveiled-gv100-gpu-and-tesla-v100-accelerator-announced\" rel=\"nofollow\">https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;11367&#x2F;nvidia-volta-unveiled-g...</a><p>- NVIDIA&#x27;s V100 GPU architecture white paper: <a href=\"http:&#x2F;&#x2F;www.nvidia.com&#x2F;object&#x2F;volta-architecture-whitepaper.html\" rel=\"nofollow\">http:&#x2F;&#x2F;www.nvidia.com&#x2F;object&#x2F;volta-architecture-whitepaper.h...</a><p>- The numbers above were using my PyTorch code at <a href=\"https:&#x2F;&#x2F;github.com&#x2F;salesforce&#x2F;awd-lstm-lm\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;salesforce&#x2F;awd-lstm-lm</a> and the Quasi-Recurrent Neural Network (QRNN) at <a href=\"https:&#x2F;&#x2F;github.com&#x2F;salesforce&#x2F;pytorch-qrnn\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;salesforce&#x2F;pytorch-qrnn</a> which features a custom CUDA kernel for speed", "Whilst it might be a general concept, the most recent time I heard your paraphrased notes on warm introductions was Marc Andreessen at Startup School.<p>Both the video and the transcript (with the warm introduction question first) are available at:<p><a href=\"http:&#x2F;&#x2F;blog.ycombinator.com&#x2F;marc-andreessen-at-startup-school&#x2F;\" rel=\"nofollow\">http:&#x2F;&#x2F;blog.ycombinator.com&#x2F;marc-andreessen-at-startup-schoo...</a>", "Facebook are getting straight up annoying when it comes to their ads. The worst for me is their in feed video player. When an ad appears on a video, you can&#x27;t like, comment, or share the video[1]. You&#x27;re going to break my flow of using your own site so you can giddily inform advertisers that I paid more attention to their ad?<p>I think Facebook have rapidly transitioned to an ad first, rather than user first, experience.<p>[1]: Screenshot - <a href=\"http:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;hCxOT\" rel=\"nofollow\">http:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;hCxOT</a>", "In practice, images are not particularly large and a batch of them would easily fit on a single GPU. What&#x27;s more common is either (a) performing the forward and backward passes on 4 GPUs where each GPU has its own batch, then collecting the gradient from all 4 backward passes or (b) splitting the computation for individual layers across multiple GPUs.<p>Both (a) and (b) have various trade-offs. Some models perform worse with large batch sizes, so (a) is not preferred, and others are hard or impossible to parallelize at the layer level, ruling out (b). Google NMT did (b), though it required many trade-offs and restrictions (see my blog post[1]), while many image based tasks are happy with large batch sizes so go with (a).<p>[1]: <a href=\"http:&#x2F;&#x2F;smerity.com&#x2F;articles&#x2F;2016&#x2F;google_nmt_arch.html\" rel=\"nofollow\">http:&#x2F;&#x2F;smerity.com&#x2F;articles&#x2F;2016&#x2F;google_nmt_arch.html</a>", "Hey Aidan,<p>As noted, it&#x27;s not a criticism of a reading list tailored for you, I simply feel it&#x27;s a really confusing reading list given the likely audience that will arrive there from Hacker News and the title. I also don&#x27;t think it can be pieced together to form a good introductory reading list or that the contents as stated are &quot;foundational&quot; yet given the progression and holes.<p>Your (1) confuses me still - I admittedly don&#x27;t know how you got started in mathematics or how that&#x27;s relevant here?<p>Regarding (2), there certainly is a place for neuroscience to influencing thinking or introduce new ideas, and I&#x27;m certainly not disregarding the entirety of that potential intersection, but it&#x27;s a very specific field that is still largely disconnected from the practical application of neural networks. This may change, slightly or substantially, in the future, but thinking again with the lens of the audience of Hacker News (where the incorrect adages of the style &quot;deep learning learns just like a human brain&quot; gets thrown around frequently), I rarely want to exaggerate the influence that neuroscience has on the field at this stage. I&#x27;d also lightly note that appealing to authority isn&#x27;t an argument, though do note the author&#x27;s pedigree.", "With all due respect, this is a quite random reading list, and appears more a result of the &#x27;if &quot;deep learning&quot; in post.title: post.upvote()&#x27; trend on Hacker News ...<p>I will pick on two under the &quot;Classics&quot; section simply as I know the author and have used their work, so am not in any way saying the work isn&#x27;t useful (it certainly can be in the right spot!), but it isn&#x27;t &quot;classic&quot; or what I&#x27;d recommend for early readers at all.\n&quot;Uncertainty in Deep Learning&quot; and &quot;Dropout as a Bayesian Approximation&quot; were both published within the last year and a half and is a PhD thesis + paper on interpretations of neural networks in a Bayesian fashion. &quot;Classic&quot; for a paper + thesis less than two years old is quite a stretch even for the fast moving field of deep learning.\nThe same holds true for many of the other papers in &quot;Classics&quot; such as batch norm which is (a) recent and (b) certainly not the only of such techniques (see layer norm, recurrent batch norm, ...) and (c) has complications in implementation[1].<p>As the simplest example, why is the original dropout paper[7] not under classics? It&#x27;s an elegant paper, fundamentally important for current neural networks, and is more classic than &quot;Dropout as a Bayesian Approximation&quot; or &quot;Dropout Rademacher Complexity of Deep Neural Networks&quot; which are both listed.<p>I&#x27;m also highly dubious of the noted neuroscience connection - most deep learning researchers use very little from neuroscience.<p>Again, this list may be helpful to the creator of the repo and tailored toward their specific research direction but it is not useful for readers from Hacker News or those aiming to get their start in deep learning. Why so many upvotes? Zero comments? Zero discussion?<p>If you want a book, check out the Deep Learning book[2]. If you want a course for RNNs, check out CS224d[3]. If you want a course for CNNs, check out CS231n[4]. If you want to get down and dirty in a practical software engineering way, check out Fast AI[5]. If you want summaries of select recent deep learning papers in GitHub format, check out Denny Britz&#x27;s notes[8]. There are many other starting points but those are my default suggestions.<p>If you really want to start learning, this isn&#x27;t the right list for you and I&#x27;d really like to suggest a more sane and potentially tailored path. Seriously. If you reply with what you want, I&#x27;ll do my best to suggest a starting point.<p>Background: I&#x27;m a deep learning researcher who publishes papers and articles[6].<p>[1]: <a href=\"http:&#x2F;&#x2F;www.alexirpan.com&#x2F;2017&#x2F;04&#x2F;26&#x2F;perils-batch-norm.html\" rel=\"nofollow\">http:&#x2F;&#x2F;www.alexirpan.com&#x2F;2017&#x2F;04&#x2F;26&#x2F;perils-batch-norm.html</a><p>[2]: <a href=\"http:&#x2F;&#x2F;www.deeplearningbook.org&#x2F;\" rel=\"nofollow\">http:&#x2F;&#x2F;www.deeplearningbook.org&#x2F;</a><p>[3]: <a href=\"http:&#x2F;&#x2F;cs224d.stanford.edu&#x2F;\" rel=\"nofollow\">http:&#x2F;&#x2F;cs224d.stanford.edu&#x2F;</a><p>[4]: <a href=\"http:&#x2F;&#x2F;cs231n.github.io&#x2F;\" rel=\"nofollow\">http:&#x2F;&#x2F;cs231n.github.io&#x2F;</a><p>[5]: <a href=\"http:&#x2F;&#x2F;course.fast.ai&#x2F;\" rel=\"nofollow\">http:&#x2F;&#x2F;course.fast.ai&#x2F;</a><p>[6]: <a href=\"http:&#x2F;&#x2F;smerity.com&#x2F;articles&#x2F;2016&#x2F;google_nmt_arch.html\" rel=\"nofollow\">http:&#x2F;&#x2F;smerity.com&#x2F;articles&#x2F;2016&#x2F;google_nmt_arch.html</a><p>[7]: <a href=\"https:&#x2F;&#x2F;www.cs.toronto.edu&#x2F;~hinton&#x2F;absps&#x2F;JMLRdropout.pdf\" rel=\"nofollow\">https:&#x2F;&#x2F;www.cs.toronto.edu&#x2F;~hinton&#x2F;absps&#x2F;JMLRdropout.pdf</a><p>[8]: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;dennybritz&#x2F;deeplearning-papernotes\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;dennybritz&#x2F;deeplearning-papernotes</a>", "I presume you&#x27;re already familiar with computing the numerical and analytical Jacobian[1][2] and just wishing for a better way? :) They&#x27;re memory intensive as all hell and pretty finicky but at least it&#x27;s something. I&#x27;ll admit that when floating point calculations are involved it can all go to hell anyway.<p>Recently I had to implement gradient calculations by hand recently (writing custom CUDA code) and had a pretty terrible time. Mixing the complications of CUDA code with my iffy manual differentiations and floating point silliness can drive you a little bonkers. I ended up implementing a slow automatic differentiated version and compared resulting outputs and gradients to help work through my bugs.<p>Here&#x27;s hoping that Tensorflow&#x27;s XLA and other JIT style CUDA compilers&#x2F;optimizers will make much of this obsolete in the near future.<p>For those not familiar, the overhead for calling a CUDA kernel can be insanely high, especially when you&#x27;re just doing an elementwise operation such as an add. Given your neural network likely has many many of these, wrapping many of these into one small piece of custom CUDA can result in substantial speed increases. Unfortunately there&#x27;s not really any automatic way of doing that yet. We&#x27;re stuck in the days of either writing manual assembly or being fine with suboptimal compiled C.<p>[1]: <a href=\"https:&#x2F;&#x2F;www.tensorflow.org&#x2F;versions&#x2F;r0.11&#x2F;api_docs&#x2F;python&#x2F;test&#x2F;gradient_checking\" rel=\"nofollow\">https:&#x2F;&#x2F;www.tensorflow.org&#x2F;versions&#x2F;r0.11&#x2F;api_docs&#x2F;python&#x2F;te...</a><p>[2]: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;pytorch&#x2F;blob&#x2F;master&#x2F;torch&#x2F;autograd&#x2F;gradcheck.py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;pytorch&#x2F;blob&#x2F;master&#x2F;torch&#x2F;autogra...</a>"]}
